Got it â€” hereâ€™s the **raw Markdown text** (no code block wrapper).
You can now directly select, copy, and paste this straight into your `README.md` file in your editor.

---

# âš¡ Building a High-Frequency Trading System â€” An Experimental Journey

> This repository documents my exploration into building and optimizing a high-frequency trading (HFT) system from scratch â€” starting from a naive prototype and gradually evolving toward hardware-accelerated performance.
>
> Itâ€™s not about writing the fastest code right away. Itâ€™s about *learning what actually makes code fast* â€” by measuring, profiling, and experimenting at every layer of the stack.
>
> Iâ€™ll be following data, not dogma â€” if the results point in a different direction than planned, Iâ€™ll go there.

---

## ğŸ§­ Motivation

The projects Iâ€™ve enjoyed most in my career were the ones where I had to **squeeze every last bit of performance out of the hardware** â€”
whether it was making **payment terminals** process transactions as fast as physically possible,
or getting the **brain of a self-driving robot** to run on a tiny **Arduino** with barely any resources.

That process â€” finding the limits of what hardware can do and bending them just a little further â€” is what Iâ€™ve always loved most.
This project is an extension of that passion: taking the same mindset and applying it to the extreme performance world of HFT.

---

## ğŸ§© Current Architecture (work-in-progress)

```mermaid
flowchart LR
  FeedSource(Bitvavo / FIXSim / Other) --> |WebSocket / TCP| QuotesObtainer
  QuotesObtainer --> OrderBook
  OrderBook --> TradingLogic
  TradingLogic --> |Buy/Sell| OrderSender
```

---

## ğŸ§ª Phase 1 â€” Naive Implementation (Baseline)

> â€œFirst make it run.â€

* Runs locally on macOS
* Focused purely on **functionality and structure**
* Uses standard STL containers (`std::vector`, `std::map`)
* Minimal threading, single market feed
* CLI visualization of order-book and trade flow
* Basic tick-to-trade latency logging

### ğŸ¯ Goals

* Build a minimal but complete **end-to-end data flow**:
  **feed â†’ order book â†’ trading logic â†’ simulated trade**
* Establish a baseline before optimizing anything
* Record first timing measurements (approximate but illustrative)

<details>
<summary>ğŸ§¾ Reflection (to be filled after completion)</summary>

* What worked well in the architecture?
* Where did latency first appear?
* Did the system behave predictably under load?
* What surprised me?

</details>

---

## ğŸ§° Phase 2 â€” Dedicated Hardware Setup (Ubuntu Environment)

> â€œBefore optimizing code, optimize your environment.â€

Move immediately to **dedicated Linux hardware** for reproducible, controllable performance testing.

### ğŸ–¥ï¸ Hardware Plan

* Linux workstation (Ubuntu)

    * Mellanox ConnectX-4/5 NIC
    * Low-latency tuned kernel
    * Fast NVMe + ample RAM
* BIOS / OS tuning

    * Disable C-states & hyper-threading
    * Fix CPU frequency
    * Configure IRQ affinity and `isolcpus`
* Software setup

    * `perf`, FlameGraph, Intel VTune
    * DPDK / AF_XDP (for later phases)
    * `taskset`, `numactl`, PTP for precise timestamps

### ğŸ§­ Journey

* Researching and buying the hardware
* Kernel and BIOS tuning logs
* Baseline latency benchmarks on clean Ubuntu
* Automation scripts for repeatable experiments

<details>
<summary>ğŸ§¾ Reflection (to be filled after completion)</summary>

* What configuration changes gave measurable differences?
* What setup challenges or unexpected bottlenecks appeared?
* How reproducible are the results compared to macOS?

</details>

---

## ğŸ” Phase 3 â€” Profiling & First Optimizations

> â€œLet the profiler guide you, not your intuition.â€

### Tools

`perf record`, `perf stat`, FlameGraphs, Intel VTune

### Experiments

* Swap STL containers for cache-optimized ones (`boost::unordered_flat_map`, `tsl::robin_map`)
* Reduce allocations (custom allocators / arena pools)
* Introduce **lock-free SPSC queues** between producer / consumer threads
* Optimize parsing pipeline (SIMD, pre-allocated buffers)
* Measure cost of synchronization (`std::mutex` vs atomics)

<details>
<summary>ğŸ§¾ Reflection (to be filled after completion)</summary>

* Which profiler insights were most actionable?
* Did any â€œobviousâ€ optimizations have no measurable effect?
* What patterns emerged across runs?

</details>

---

## ğŸ§µ Phase 4 â€” Core-Level and Memory Optimizations

> â€œHow close can I get to the metal on general-purpose CPUs?â€

* Thread pinning with `taskset`
* NUMA-aware allocation (`numactl`, hugepages)
* Busy-wait vs epoll polling
* Pre-faulted memory, page coloring
* Cache-aligned structures / false-sharing elimination
* Custom per-thread allocators
* Branchless logic and `__builtin_expect` hints

<details>
<summary>ğŸ§¾ Reflection (to be filled after completion)</summary>

* Which micro-optimizations scaled best?
* Did any introduce instability or regressions?
* How much lower is the tail latency now?

</details>

---

## ğŸ§± Phase 5 â€” Hardware Acceleration (FPGA Exploration)

> â€œWhen software hits the wall.â€

* Simple Verilog feed parser for UDP packets
* DMA into host memory
* Compare FPGA vs CPU latency
* Hybrid setup: CPU order logic + FPGA feed handling

<details>
<summary>ğŸ§¾ Reflection (to be filled after completion)</summary>

* What parts of the system map cleanly to hardware?
* What trade-offs appear between determinism and flexibility?
* How maintainable is the hardware pipeline?

</details>

---

## ğŸ“ˆ Progress Tracking (to be updated)

| Phase               | Avg Tick-to-Trade (Âµs) | 99th Percentile (Âµs) | Notes                       |
| :------------------ | ---------------------: | -------------------: | :-------------------------- |
| 1 â€“ Naive           |                    TBD |                  TBD | macOS baseline              |
| 2 â€“ Ubuntu baseline |                    TBD |                  TBD | Clean kernel setup          |
| 3 â€“ Profiled        |                    TBD |                  TBD | First optimizations         |
| 4 â€“ Core tuned      |                    TBD |                  TBD | Thread pinning / allocators |
| 5 â€“ FPGA            |                    TBD |                  TBD | Hardware offload            |

---

## ğŸ§° Toolchain

* **C++20**, CMake, Conan
* **Profilers:** `perf`, FlameGraph, VTune
* **Libraries:** Boost, fmt, spdlog
* **Network Frameworks:** DPDK / AF_XDP
* **Hardware:** Mellanox NIC, DE10-Nano FPGA
* **Visualization:** CLI dashboard, Grafana metrics

---

## ğŸ“š Learning Objectives

* Understand the full stack of low-latency systems
* Learn to measure rather than guess
* Build intuition for caches, NUMA, and scheduling
* Explore kernel-bypass networking
* Gain hands-on experience with hardware acceleration

---

## ğŸ§© Future Ideas (Data-Driven)

* Multi-exchange aggregation
* Adaptive batching strategies
* Risk throttling subsystem
* Microsecond-level timestamp correlation
* Cross-machine latency measurement

---

## ğŸ’¬ Philosophy

> â€œMeasure first, optimize second, automate third.â€

This is an **open-ended experiment**, not a predefined tutorial.
The direction may change at any point if the data shows something unexpected â€” and thatâ€™s the fun of it.
